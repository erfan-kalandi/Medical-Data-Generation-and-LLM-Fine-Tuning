{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IqudpJ_tHLF"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, TrainingArguments, DataCollatorForLanguageModeling, BitsAndBytesConfig\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from datasets import load_dataset, Dataset\n",
        "import torch\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLZudc1atNtD"
      },
      "outputs": [],
      "source": [
        "with open(\"symptoms_diagnoses_100.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "def format_example(example):\n",
        "    return {\n",
        "        \"text\": f\"علائم بیمار: {example['symptom']}\\nتشخیص: {example['diagnosis']}\"\n",
        "    }\n",
        "\n",
        "dataset = Dataset.from_list(data)\n",
        "dataset = dataset.map(format_example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1xKVe9bufg7"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNyP69oktRIK"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"unsloth/Llama-3.2-1B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xhyculOvMBy"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, tokenizer):\n",
        "    correct = 0\n",
        "    results = []\n",
        "\n",
        "    with open(\"qa.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        questions = json.load(f)\n",
        "\n",
        "    for q in questions:\n",
        "        prompt = f\"\"\"در ادامه یک سؤال چهارگزینه‌ای پزشکی آمده است. فقط یکی از گزینه‌ها صحیح است. لطفاً فقط شماره گزینه صحیح را بنویس.\\n\\nسؤال: {q[\"question\"]}\\n\"\"\"\n",
        "        for i, opt in enumerate(q[\"options\"], 1):\n",
        "            prompt += f\"{i}. {opt}\\n\"\n",
        "        prompt += \"\\nپاسخ:\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=20,\n",
        "                do_sample=False\n",
        "            )\n",
        "\n",
        "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        predicted_text = decoded.split(\"پاسخ:\")[-1].strip()\n",
        "        predicted_answer = predicted_text.split(\"\\n\")[0]\n",
        "\n",
        "        try:\n",
        "            is_correct = int(re.sub(r\"\\D\", \"\", predicted_answer)) == q[\"answer\"]\n",
        "        except:\n",
        "            is_correct = False\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "\n",
        "        results.append({\n",
        "            \"question\": q[\"question\"],\n",
        "            \"predicted\": predicted_answer,\n",
        "            \"correct\": q[\"answer\"],\n",
        "            \"is_correct\": is_correct\n",
        "        })\n",
        "\n",
        "    accuracy = correct / len(questions)\n",
        "    return accuracy, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvyZYUIMwEFJ"
      },
      "outputs": [],
      "source": [
        "acc_before, results_before = evaluate_model(model, tokenizer)\n",
        "print(f\"دقت مدل پایه: {acc_before:.2%}\")\n",
        "\n",
        "with open(\"evaluation_base_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_before, f, ensure_ascii=False, indent=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsLgWec4XZ6T"
      },
      "outputs": [],
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\" تعداد پارامترهای قابل آموزش: {trainable_params:,}\")\n",
        "print(f\"درصد از کل مدل: {100 * trainable_params / total_params:.2f}%\")"
      ],
      "metadata": {
        "id": "DtfAVsmVF3a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jStuQh4BaLHR"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=384)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vz31n4ramdx"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=4e-5,\n",
        "    logging_dir=\"./logs\",\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    fp16=False,\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"./fine-tuned-model\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7RcFABquvYo"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwZHTpWE7Lhu"
      },
      "outputs": [],
      "source": [
        "acc_after, results_after = evaluate_model(model, tokenizer)\n",
        "print(f\"دقت مدل fine-tuned: {acc_after:.2%}\")\n",
        "\n",
        "with open(\"evaluation_finetuned_model.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results_after, f, ensure_ascii=False, indent=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}